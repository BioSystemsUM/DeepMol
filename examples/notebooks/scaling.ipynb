{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Data Scaling with DeepMol\n",
    "\n",
    "In machine learning and deep learning, scaling is important because it can significantly affect the performance of the algorithms. This is also true in the field of chemoinformatics, which involves the use of machine learning and other computational methods to analyze chemical data.\n",
    "\n",
    "One reason why scaling is important is that many machine learning algorithms use distance-based measures to calculate similarities between data points. If the features of the data are not scaled, the algorithm may give more weight to features with larger values, even if they are not more important for the analysis. This can lead to biased results and suboptimal model performance.\n",
    "\n",
    "Another reason why scaling is important is that it can help to speed up the training process. When the features of the data are not scaled, the optimization algorithm used in training may take longer to converge or may even fail to converge at all. This can be especially problematic in deep learning, where large amounts of data and complex models are often used.\n",
    "\n",
    "As we will see below, DeepMol offers a wide variety of scalers."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Let's start by loading some data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 14:38:03.499258: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-30 14:38:03.582566: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-05-30 14:38:03.582578: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-05-30 14:38:04.047547: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-05-30 14:38:04.047599: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-05-30 14:38:04.047605: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "Skipped loading modules with pytorch-geometric dependency, missing a dependency. No module named 'torch_geometric'\n",
      "Skipped loading modules with pytorch-geometric dependency, missing a dependency. cannot import name 'DMPNN' from 'deepchem.models.torch_models' (/home/bisbii/anaconda3/envs/deepmol/lib/python3.8/site-packages/deepchem/models/torch_models/__init__.py)\n",
      "Skipped loading modules with pytorch-lightning dependency, missing a dependency. No module named 'pytorch_lightning'\n",
      "Skipped loading some Jax models, missing a dependency. No module named 'jax'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-05-30 14:38:05,632 — INFO — Assuming classification since there are less than 10 unique y values. If otherwise, explicitly set the mode to 'regression'!\n"
     ]
    }
   ],
   "source": [
    "from deepmol.compound_featurization import TwoDimensionDescriptors\n",
    "from deepmol.loaders import CSVLoader\n",
    "\n",
    "# Load data from CSV file\n",
    "loader = CSVLoader(dataset_path='../data/CHEMBL217_reduced.csv',\n",
    "                   smiles_field='SMILES',\n",
    "                   id_field='Original_Entry_ID',\n",
    "                   labels_fields=['Activity_Flag'],\n",
    "                   mode='auto',\n",
    "                   shard_size=2500)\n",
    "# create the dataset\n",
    "data = loader.create_dataset(sep=',', header=0)\n",
    "# create the features\n",
    "TwoDimensionDescriptors().featurize(data, inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-30T13:38:09.039016509Z",
     "start_time": "2023-05-30T13:38:03.055617994Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 1.2918277e+01,  1.7210670e-02,  1.2918277e+01, ...,\n         0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n       [ 1.3306055e+01, -5.6191501e-03,  1.3306055e+01, ...,\n         0.0000000e+00,  0.0000000e+00,  1.0000000e+00],\n       [ 1.1760469e+01, -2.3133385e-01,  1.1760469e+01, ...,\n         0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n       ...,\n       [ 1.2702195e+01, -3.6049552e+00,  1.2702195e+01, ...,\n         0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n       [ 2.4115279e+00,  4.3334645e-01,  2.4115279e+00, ...,\n         0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n       [ 6.1730037e+00,  6.9678569e-01,  6.1730037e+00, ...,\n         0.0000000e+00,  0.0000000e+00,  0.0000000e+00]], dtype=float32)"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view the features\n",
    "data.X # data is very heterogeneous"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-30T13:38:09.045870088Z",
     "start_time": "2023-05-30T13:38:09.043609944Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### StandardScaler\n",
    "\n",
    "Standardize features by removing the mean and scaling to unit variance."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 0.6015701 ,  0.5215607 ,  0.6015701 , ..., -0.2591845 ,\n        -0.32493442, -0.24450661],\n       [ 0.7382216 ,  0.5070287 ,  0.7382216 , ..., -0.2591845 ,\n        -0.32493442,  4.0300846 ],\n       [ 0.19356354,  0.36335236,  0.19356354, ..., -0.2591845 ,\n        -0.32493442, -0.24450661],\n       ...,\n       [ 0.5254238 , -1.7840909 ,  0.5254238 , ..., -0.2591845 ,\n        -0.32493442, -0.24450661],\n       [-3.1009648 ,  0.78644764, -3.1009648 , ..., -0.2591845 ,\n        -0.32493442, -0.24450661],\n       [-1.7754363 ,  0.9541371 , -1.7754363 , ..., -0.2591845 ,\n        -0.32493442, -0.24450661]], dtype=float32)"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "from deepmol.scalers import StandardScaler\n",
    "\n",
    "d1 = deepcopy(data)\n",
    "scaler = StandardScaler() # Standardize features by removing the mean and scaling to unit variance.\n",
    "scaler.fit_transform(d1, inplace=True) # you can scale only a portion of the data by passing a columns argument with the indexes of the columns to scale\n",
    "d1.X # the data is much more homogeneous"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-30T13:38:28.289244190Z",
     "start_time": "2023-05-30T13:38:28.217374996Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### MinMaxScaler\n",
    "\n",
    "Transform features by scaling each feature to a given range."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 1.2589104 ,  1.5757873 ,  1.2589104 , ..., -2.        ,\n        -2.        , -2.        ],\n       [ 1.3791888 ,  1.567372  ,  1.3791888 , ..., -2.        ,\n        -2.        ,  0.        ],\n       [ 0.8997896 ,  1.4841713 ,  0.8997896 , ..., -2.        ,\n        -2.        , -2.        ],\n       ...,\n       [ 1.1918876 ,  0.24062085,  1.1918876 , ..., -2.        ,\n        -2.        , -2.        ],\n       [-2.        ,  1.729179  , -2.        , ..., -2.        ,\n        -2.        , -2.        ],\n       [-0.83329153,  1.8262854 , -0.83329153, ..., -2.        ,\n        -2.        , -2.        ]], dtype=float32)"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from deepmol.scalers import MinMaxScaler\n",
    "\n",
    "d2 = deepcopy(data)\n",
    "scaler = MinMaxScaler(feature_range=(-2, 2))\n",
    "scaler.fit_transform(d2, inplace=True)\n",
    "d2.X # data is scaled between -2 and 2"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-30T13:38:40.594323466Z",
     "start_time": "2023-05-30T13:38:40.550528796Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### MaxAbsScaler\n",
    "\n",
    "Scale each feature by its maximum absolute value."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 8.4391510e-01,  1.7773149e-03,  8.4391510e-01, ...,\n         0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n       [ 8.6924756e-01, -5.8027951e-04,  8.6924756e-01, ...,\n         0.0000000e+00,  0.0000000e+00,  5.0000000e-01],\n       [ 7.6827878e-01, -2.3889430e-02,  7.6827878e-01, ...,\n         0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n       ...,\n       [ 8.2979906e-01, -3.7227723e-01,  8.2979906e-01, ...,\n         0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n       [ 1.5753841e-01,  4.4750907e-02,  1.5753841e-01, ...,\n         0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n       [ 4.0326515e-01,  7.1955800e-02,  4.0326515e-01, ...,\n         0.0000000e+00,  0.0000000e+00,  0.0000000e+00]], dtype=float32)"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from deepmol.scalers import MaxAbsScaler\n",
    "\n",
    "d3 = deepcopy(data)\n",
    "scaler = MaxAbsScaler()\n",
    "scaler.fit_transform(d3, inplace=True)\n",
    "d3.X"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-30T13:38:51.309771171Z",
     "start_time": "2023-05-30T13:38:51.262092273Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### RobustScaler\n",
    "\n",
    "Scale features using statistics that are robust to outliers."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 0.22212751,  0.29477584,  0.22212751, ...,  0.        ,\n         0.        ,  0.        ],\n       [ 0.3872069 ,  0.26957947,  0.3872069 , ...,  0.        ,\n         0.        ,  1.        ],\n       [-0.27075756,  0.02046694, -0.27075756, ...,  0.        ,\n         0.        ,  0.        ],\n       ...,\n       [ 0.13014036, -3.7028677 ,  0.13014036, ...,  0.        ,\n         0.        ,  0.        ],\n       [-4.250654  ,  0.75404876, -4.250654  , ...,  0.        ,\n         0.        ,  0.        ],\n       [-2.649373  ,  1.0447963 , -2.649373  , ...,  0.        ,\n         0.        ,  0.        ]], dtype=float32)"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from deepmol.scalers import RobustScaler\n",
    "\n",
    "d4 = deepcopy(data)\n",
    "scaler = RobustScaler()\n",
    "scaler.fit_transform(d4, inplace=True)\n",
    "d4.X # scaled data"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-30T13:38:58.644523278Z",
     "start_time": "2023-05-30T13:38:58.560940190Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Normalizer scaler\n",
    "\n",
    "Normalize samples individually to unit norm."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 2.4346070e-07,  3.2435607e-10,  2.4346070e-07, ...,\n         0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n       [ 3.5423795e-08, -1.4959476e-11,  3.5423795e-08, ...,\n         0.0000000e+00,  0.0000000e+00,  2.6622313e-09],\n       [ 7.4276683e-04, -1.4610566e-05,  7.4276683e-04, ...,\n         0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n       ...,\n       [ 2.1571793e-06, -6.1221971e-07,  2.1571793e-06, ...,\n         0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n       [ 7.5876460e-06,  1.3634839e-06,  7.5876460e-06, ...,\n         0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n       [ 1.9041399e-05,  2.1493222e-06,  1.9041399e-05, ...,\n         0.0000000e+00,  0.0000000e+00,  0.0000000e+00]], dtype=float32)"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from deepmol.scalers import Normalizer\n",
    "\n",
    "d5 = deepcopy(data)\n",
    "scaler = Normalizer(norm='l2') # One of 'l1', 'l2' or 'max'. The norm to use to normalize each non-zero sample.\n",
    "scaler.fit_transform(d5, inplace=True)\n",
    "d5.X # scaled data"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-30T13:39:06.100131711Z",
     "start_time": "2023-05-30T13:39:06.064325319Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Binarizer scaler\n",
    "\n",
    "Binarize data (set feature values to 0 or 1) according to a threshold."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[1., 0., 1., ..., 0., 0., 0.],\n       [1., 0., 1., ..., 0., 0., 0.],\n       [1., 0., 1., ..., 0., 0., 0.],\n       ...,\n       [1., 0., 1., ..., 0., 0., 0.],\n       [1., 0., 1., ..., 0., 0., 0.],\n       [1., 0., 1., ..., 0., 0., 0.]], dtype=float32)"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from deepmol.scalers import Binarizer\n",
    "\n",
    "d6 = deepcopy(data)\n",
    "scaler = Binarizer(threshold=1) # features higher than 10 are set to 1, features lower than 10 are set to 0\n",
    "scaler.fit_transform(d6, inplace=True)\n",
    "d6.X"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-30T13:39:12.727636867Z",
     "start_time": "2023-05-30T13:39:12.678574900Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### QuantileTransformer\n",
    "\n",
    "The QuantileTransformer is a preprocessing method that transforms input data to have a specified probability distribution. This function maps the data to a uniform or normal distribution using the quantiles of the input data.\n",
    "\n",
    "This transformer is often useful when working with machine learning algorithms that are sensitive to the scale and distribution of the input data, such as neural networks. The QuantileTransformer is particularly useful when the input data has a highly skewed distribution, as it can transform the data to a more Gaussian distribution."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0.72686756, 0.7179994 , 0.72686756, ..., 0.        , 0.        ,\n        0.        ],\n       [0.86711156, 0.7053635 , 0.86711156, ..., 0.        , 0.        ,\n        0.9714715 ],\n       [0.32533428, 0.51655453, 0.32533428, ..., 0.        , 0.        ,\n        0.        ],\n       ...,\n       [0.62756836, 0.12748909, 0.62756836, ..., 0.        , 0.        ,\n        0.        ],\n       [0.        , 0.8474799 , 0.        , ..., 0.        , 0.        ,\n        0.        ],\n       [0.14120819, 0.92178524, 0.14120819, ..., 0.        , 0.        ,\n        0.        ]], dtype=float32)"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from deepmol.scalers import QuantileTransformer\n",
    "\n",
    "d7 = deepcopy(data)\n",
    "scaler = QuantileTransformer()\n",
    "scaler.fit_transform(d7, inplace=True)\n",
    "d7.X # scale data"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-30T13:39:21.404280372Z",
     "start_time": "2023-05-30T13:39:20.198889363Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### PowerTransformer\n",
    "\n",
    "The PowerTransformer is a preprocessing function in scikit-learn that applies a power transformation to make the data more Gaussian-like. It can be used for data with skewed distributions, as well as data that has a linear relationship with the target variable.\n",
    "\n",
    "The PowerTransformer applies either a Box-Cox transformation or a Yeo-Johnson transformation to the input data. The Box-Cox transformation is only applicable to positive data, while the Yeo-Johnson transformation can be applied to both positive and negative data."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bisbii/anaconda3/envs/deepmol/lib/python3.8/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply\n",
      "  x = um.multiply(x, x, out=x)\n",
      "/home/bisbii/anaconda3/envs/deepmol/lib/python3.8/site-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce\n",
      "  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)\n"
     ]
    },
    {
     "data": {
      "text/plain": "array([[ 0.6452927 ,  0.36367184,  0.6452927 , ..., -0.26236013,\n        -0.4512646 , -0.24539872],\n       [ 0.93890953,  0.33411208,  0.93890953, ..., -0.26236013,\n        -0.4512646 ,  4.075001  ],\n       [-0.0993756 ,  0.07504444, -0.0993756 , ..., -0.26236013,\n        -0.4512646 , -0.24539872],\n       ...,\n       [ 0.49173132, -1.5438824 ,  0.49173132, ..., -0.26236013,\n        -0.4512646 , -0.24539872],\n       [-1.921834  ,  1.0273771 , -1.921834  , ..., -0.26236013,\n        -0.4512646 , -0.24539872],\n       [-1.7411773 ,  1.5712001 , -1.7411773 , ..., -0.26236013,\n        -0.4512646 , -0.24539872]], dtype=float32)"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from deepmol.scalers import PowerTransformer\n",
    "\n",
    "d8 = deepcopy(data)\n",
    "scaler = PowerTransformer(method='yeo-johnson', # The power transform method. Available methods are: 'yeo-johnson', works with positive and negative values; box-cox', only works with strictly positive values\n",
    "                          standardize=True) # apply zero mean, unit variance normalization to the transformed output\n",
    "scaler.fit_transform(d8, inplace=True)\n",
    "d8.X # scaled data"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-30T13:39:26.454539555Z",
     "start_time": "2023-05-30T13:39:25.656480263Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
