{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Performing Feature Selection with DeepMol\n",
    "\n",
    "The selection of the most relevant features can significantly improve the performance of a machine learning model in chemoinformatics. By removing irrelevant or redundant features, feature selection can reduce overfitting and improve the model's ability to generalize to new data. Additionally, feature selection can reduce the computational burden of training a machine learning model by reducing the number of features that need to be processed."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "DeepMol supports many types of feature selection provided by scikit-learn including Low Variance Feature Selection, KBest, Percentile, Recursive Feature Elimination and selecting features based on importance weights."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Let's load our dataset with already computed features (2048 features)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'deepmol'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_35222/806919096.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0;32mfrom\u001B[0m \u001B[0mdeepmol\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mloaders\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mCSVLoader\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0;31m# Load data from CSV file\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m loader = CSVLoader(dataset_path='../data/example_data_with_features.csv',\n\u001B[1;32m      5\u001B[0m                    \u001B[0msmiles_field\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m'mols'\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'deepmol'"
     ]
    }
   ],
   "source": [
    "from deepmol.loaders import CSVLoader\n",
    "\n",
    "# Load data from CSV file\n",
    "loader = CSVLoader(dataset_path='../data/example_data_with_features.csv',\n",
    "                   smiles_field='mols',\n",
    "                   id_field='ids',\n",
    "                   labels_fields=['y'],\n",
    "                   features_fields=[f'feat_{i+1}' for i in range(2048)],\n",
    "                   shard_size=500,\n",
    "                   mode='auto')\n",
    "# create the dataset\n",
    "csv_dataset = loader.create_dataset(sep=',', header=0)\n",
    "csv_dataset.get_shape()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-24T11:03:55.927357540Z",
     "start_time": "2023-05-24T11:03:55.926056848Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Lets use the LowVarianceFS feature selector\n",
    "\n",
    "Low variance feature selection is a technique used to select features in a dataset that have little or no variability across the data. This method is based on the assumption that features with low variance have little impact on the model's predictive ability and can be safely removed.\n",
    "\n",
    "To apply low variance feature selection, one first calculates the variance of each feature across the entire dataset. The features with variance below a certain threshold are then removed from the dataset, typically by setting a minimum variance threshold or using a percentile of variance. The threshold value is usually determined by trial and error or through cross-validation."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'deepmol'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_35222/4136987901.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mcopy\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mdeepcopy\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0;32mfrom\u001B[0m \u001B[0mdeepmol\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfeature_selection\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mLowVarianceFS\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      3\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0;31m# make a copy of our dataset\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0md1\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdeepcopy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcsv_dataset\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'deepmol'"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "from deepmol.feature_selection import LowVarianceFS\n",
    "\n",
    "# make a copy of our dataset\n",
    "d1 = deepcopy(csv_dataset)\n",
    "\n",
    "# instantiate our feature selector\n",
    "fs = LowVarianceFS(threshold=0.15)\n",
    "# performa feature selection\n",
    "fs.select_features(d1)\n",
    "# see changes in the shape of the features\n",
    "d1.get_shape() # our dataset only has 47 features (out of 2048) with a variability higher than 15%."
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-24T13:10:51.402319613Z",
     "start_time": "2023-05-24T13:10:51.360032871Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Let's use the KbestFS feature selector\n",
    "\n",
    "SelectKBest is a feature selection algorithm in machine learning that selects the top k features with the highest predictive power from a given dataset. This algorithm works by scoring each feature and selecting the top k features based on their scores.\n",
    "\n",
    "The score of each feature is determined using a statistical test, such as the chi-squared test, mutual information, or ANOVA F-test, depending on the nature of the dataset and the problem being solved. The algorithm computes a score for each feature, ranking them in descending order. It then selects the top k features with the highest scores and discards the rest.\n",
    "\n",
    "The purpose of feature selection is to improve the model's performance by reducing the number of irrelevant or redundant features. Selecting only the most relevant features can help to reduce overfitting, increase model interpretability, and reduce computational costs."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-03-16 15:06:47,234 — INFO — Mols_shape: (500,)\n",
      "2023-03-16 15:06:47,235 — INFO — Features_shape: (500, 250)\n",
      "2023-03-16 15:06:47,236 — INFO — Labels_shape: (500,)\n"
     ]
    },
    {
     "data": {
      "text/plain": "((500,), (500, 250), (500,))"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_selection import chi2\n",
    "from deepmol.feature_selection import KbestFS\n",
    "\n",
    "# make a copy of our dataset\n",
    "d2 = deepcopy(csv_dataset)\n",
    "fs = KbestFS(k=250, score_func=chi2) # the top k features with the highest predictive power will be kept\n",
    "# perform feature selection\n",
    "fs.select_features(d2)\n",
    "d2.get_shape() # as we can see only 250 feature were kept"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Let's use the PercentilFS feature selector\n",
    "\n",
    "SelectPercentile is a feature selection algorithm in machine learning that selects the top features based on their statistical scores, similar to SelectKBest. However, instead of selecting a fixed number of features, SelectPercentile selects a percentage of the most informative features from a given dataset.\n",
    "\n",
    "The main advantage of SelectPercentile over SelectKBest is that it adapts to datasets of different sizes, so it can select an appropriate number of features for datasets with different numbers of features."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-03-16 15:07:09,364 — INFO — Mols_shape: (500,)\n",
      "2023-03-16 15:07:09,365 — INFO — Features_shape: (500, 204)\n",
      "2023-03-16 15:07:09,366 — INFO — Labels_shape: (500,)\n"
     ]
    },
    {
     "data": {
      "text/plain": "((500,), (500, 204), (500,))"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from deepmol.feature_selection import PercentilFS\n",
    "\n",
    "# make a copy of our dataset\n",
    "d3 = deepcopy(csv_dataset)\n",
    "fs = PercentilFS(percentil=10, score_func=chi2) # keep the 10 percent top predictive features\n",
    "fs.select_features(d3)\n",
    "d3.get_shape() # 10 percent of 2048 features --> 204 features were kept"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Let's use the RFECVFS feature selector\n",
    "\n",
    "Recursive Feature Elimination with Cross-Validation (RFECV) is a feature selection algorithm in machine learning that selects the most informative subset of features from a given dataset by iteratively eliminating the least important features.\n",
    "\n",
    "RFECV uses a machine learning model (e.g., linear regression, logistic regression, or support vector machine) to rank the importance of each feature in the dataset. It then eliminates the feature with the lowest importance score, re-evaluates the performance of the model, and repeats the process until a specified number of features is reached.\n",
    "\n",
    "The cross-validation (CV) component of RFECV involves dividing the dataset into k-folds, training the model on k-1 folds, and evaluating it on the remaining fold. This process is repeated k times, with each fold serving as the test set once. The performance of the model is then averaged over the k-folds, providing a more reliable estimate of model performance."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting estimator with 2048 features.\n",
      "Fitting estimator with 2038 features.\n",
      "Fitting estimator with 2028 features.\n",
      "Fitting estimator with 2018 features.\n",
      "Fitting estimator with 2008 features.\n",
      "Fitting estimator with 1998 features.\n",
      "Fitting estimator with 1988 features.\n",
      "Fitting estimator with 1978 features.\n",
      "Fitting estimator with 1968 features.\n",
      "Fitting estimator with 1958 features.\n",
      "Fitting estimator with 1948 features.\n",
      "Fitting estimator with 1938 features.\n",
      "Fitting estimator with 1928 features.\n",
      "Fitting estimator with 1918 features.\n",
      "Fitting estimator with 1908 features.\n",
      "Fitting estimator with 1898 features.\n",
      "Fitting estimator with 1888 features.\n",
      "Fitting estimator with 1878 features.\n",
      "Fitting estimator with 1868 features.\n",
      "Fitting estimator with 1858 features.\n",
      "Fitting estimator with 1848 features.\n",
      "Fitting estimator with 1838 features.\n",
      "Fitting estimator with 1828 features.\n",
      "Fitting estimator with 1818 features.\n",
      "Fitting estimator with 1808 features.\n",
      "Fitting estimator with 1798 features.\n",
      "Fitting estimator with 1788 features.\n",
      "Fitting estimator with 1778 features.\n",
      "Fitting estimator with 1768 features.\n",
      "Fitting estimator with 1758 features.\n",
      "Fitting estimator with 1748 features.\n",
      "Fitting estimator with 1738 features.\n",
      "Fitting estimator with 1728 features.\n",
      "Fitting estimator with 1718 features.\n",
      "Fitting estimator with 1708 features.\n",
      "Fitting estimator with 1698 features.\n",
      "Fitting estimator with 1688 features.\n",
      "Fitting estimator with 1678 features.\n",
      "Fitting estimator with 1668 features.\n",
      "Fitting estimator with 1658 features.\n",
      "Fitting estimator with 1648 features.\n",
      "Fitting estimator with 1638 features.\n",
      "Fitting estimator with 1628 features.\n",
      "Fitting estimator with 1618 features.\n",
      "Fitting estimator with 1608 features.\n",
      "Fitting estimator with 1598 features.\n",
      "Fitting estimator with 1588 features.\n",
      "Fitting estimator with 1578 features.\n",
      "Fitting estimator with 1568 features.\n",
      "Fitting estimator with 1558 features.\n",
      "Fitting estimator with 1548 features.\n",
      "Fitting estimator with 1538 features.\n",
      "Fitting estimator with 1528 features.\n",
      "Fitting estimator with 1518 features.\n",
      "Fitting estimator with 1508 features.\n",
      "Fitting estimator with 1498 features.\n",
      "Fitting estimator with 1488 features.\n",
      "Fitting estimator with 1478 features.\n",
      "Fitting estimator with 1468 features.\n",
      "Fitting estimator with 1458 features.\n",
      "Fitting estimator with 1448 features.\n",
      "Fitting estimator with 1438 features.\n",
      "Fitting estimator with 1428 features.\n",
      "Fitting estimator with 1418 features.\n",
      "Fitting estimator with 1408 features.\n",
      "Fitting estimator with 1398 features.\n",
      "Fitting estimator with 1388 features.\n",
      "Fitting estimator with 1378 features.\n",
      "Fitting estimator with 1368 features.\n",
      "Fitting estimator with 1358 features.\n",
      "Fitting estimator with 1348 features.\n",
      "Fitting estimator with 1338 features.\n",
      "Fitting estimator with 1328 features.\n",
      "Fitting estimator with 1318 features.\n",
      "Fitting estimator with 1308 features.\n",
      "Fitting estimator with 1298 features.\n",
      "Fitting estimator with 1288 features.\n",
      "Fitting estimator with 1278 features.\n",
      "Fitting estimator with 1268 features.\n",
      "Fitting estimator with 1258 features.\n",
      "Fitting estimator with 1248 features.\n",
      "Fitting estimator with 1238 features.\n",
      "Fitting estimator with 1228 features.\n",
      "Fitting estimator with 1218 features.\n",
      "Fitting estimator with 1208 features.\n",
      "Fitting estimator with 1198 features.\n",
      "Fitting estimator with 1188 features.\n",
      "Fitting estimator with 1178 features.\n",
      "Fitting estimator with 1168 features.\n",
      "Fitting estimator with 1158 features.\n",
      "Fitting estimator with 1148 features.\n",
      "Fitting estimator with 1138 features.\n",
      "Fitting estimator with 1128 features.\n",
      "Fitting estimator with 1118 features.\n",
      "Fitting estimator with 1108 features.\n",
      "Fitting estimator with 1098 features.\n",
      "Fitting estimator with 1088 features.\n",
      "Fitting estimator with 1078 features.\n",
      "Fitting estimator with 1068 features.\n",
      "Fitting estimator with 1058 features.\n",
      "Fitting estimator with 1048 features.\n",
      "Fitting estimator with 1038 features.\n",
      "Fitting estimator with 1028 features.\n",
      "Fitting estimator with 2048 features.\n",
      "Fitting estimator with 2038 features.\n",
      "Fitting estimator with 2028 features.\n",
      "Fitting estimator with 2018 features.\n",
      "Fitting estimator with 2008 features.\n",
      "Fitting estimator with 1998 features.\n",
      "Fitting estimator with 1988 features.\n",
      "Fitting estimator with 1978 features.\n",
      "Fitting estimator with 1968 features.\n",
      "Fitting estimator with 1958 features.\n",
      "Fitting estimator with 1948 features.\n",
      "Fitting estimator with 1938 features.\n",
      "Fitting estimator with 1928 features.\n",
      "Fitting estimator with 1918 features.\n",
      "Fitting estimator with 1908 features.\n",
      "Fitting estimator with 1898 features.\n",
      "Fitting estimator with 1888 features.\n",
      "Fitting estimator with 1878 features.\n",
      "Fitting estimator with 1868 features.\n",
      "Fitting estimator with 1858 features.\n",
      "Fitting estimator with 1848 features.\n",
      "Fitting estimator with 1838 features.\n",
      "Fitting estimator with 1828 features.\n",
      "Fitting estimator with 1818 features.\n",
      "Fitting estimator with 1808 features.\n",
      "Fitting estimator with 1798 features.\n",
      "Fitting estimator with 1788 features.\n",
      "Fitting estimator with 1778 features.\n",
      "Fitting estimator with 1768 features.\n",
      "Fitting estimator with 1758 features.\n",
      "Fitting estimator with 1748 features.\n",
      "Fitting estimator with 1738 features.\n",
      "Fitting estimator with 1728 features.\n",
      "Fitting estimator with 1718 features.\n",
      "Fitting estimator with 1708 features.\n",
      "Fitting estimator with 1698 features.\n",
      "Fitting estimator with 1688 features.\n",
      "Fitting estimator with 1678 features.\n",
      "Fitting estimator with 1668 features.\n",
      "Fitting estimator with 1658 features.\n",
      "Fitting estimator with 1648 features.\n",
      "Fitting estimator with 1638 features.\n",
      "Fitting estimator with 1628 features.\n",
      "Fitting estimator with 1618 features.\n",
      "Fitting estimator with 1608 features.\n",
      "Fitting estimator with 1598 features.\n",
      "Fitting estimator with 1588 features.\n",
      "Fitting estimator with 1578 features.\n",
      "Fitting estimator with 1568 features.\n",
      "Fitting estimator with 1558 features.\n",
      "Fitting estimator with 1548 features.\n",
      "Fitting estimator with 1538 features.\n",
      "Fitting estimator with 1528 features.\n",
      "Fitting estimator with 1518 features.\n",
      "Fitting estimator with 1508 features.\n",
      "Fitting estimator with 1498 features.\n",
      "Fitting estimator with 1488 features.\n",
      "Fitting estimator with 1478 features.\n",
      "Fitting estimator with 1468 features.\n",
      "Fitting estimator with 1458 features.\n",
      "Fitting estimator with 1448 features.\n",
      "Fitting estimator with 1438 features.\n",
      "Fitting estimator with 1428 features.\n",
      "Fitting estimator with 1418 features.\n",
      "Fitting estimator with 1408 features.\n",
      "Fitting estimator with 1398 features.\n",
      "Fitting estimator with 1388 features.\n",
      "Fitting estimator with 1378 features.\n",
      "Fitting estimator with 1368 features.\n",
      "Fitting estimator with 1358 features.\n",
      "Fitting estimator with 1348 features.\n",
      "Fitting estimator with 1338 features.\n",
      "Fitting estimator with 1328 features.\n",
      "Fitting estimator with 1318 features.\n",
      "Fitting estimator with 1308 features.\n",
      "Fitting estimator with 1298 features.\n",
      "Fitting estimator with 1288 features.\n",
      "Fitting estimator with 1278 features.\n",
      "Fitting estimator with 1268 features.\n",
      "Fitting estimator with 1258 features.\n",
      "Fitting estimator with 1248 features.\n",
      "Fitting estimator with 1238 features.\n",
      "Fitting estimator with 1228 features.\n",
      "Fitting estimator with 1218 features.\n",
      "Fitting estimator with 1208 features.\n",
      "Fitting estimator with 1198 features.\n",
      "Fitting estimator with 1188 features.\n",
      "Fitting estimator with 1178 features.\n",
      "Fitting estimator with 1168 features.\n",
      "Fitting estimator with 1158 features.\n",
      "Fitting estimator with 1148 features.\n",
      "Fitting estimator with 1138 features.\n",
      "Fitting estimator with 1128 features.\n",
      "Fitting estimator with 1118 features.\n",
      "Fitting estimator with 1108 features.\n",
      "Fitting estimator with 1098 features.\n",
      "Fitting estimator with 1088 features.\n",
      "Fitting estimator with 1078 features.\n",
      "Fitting estimator with 1068 features.\n",
      "Fitting estimator with 1058 features.\n",
      "Fitting estimator with 1048 features.\n",
      "Fitting estimator with 1038 features.\n",
      "Fitting estimator with 1028 features.\n",
      "Fitting estimator with 2048 features.\n",
      "Fitting estimator with 2038 features.\n",
      "Fitting estimator with 2028 features.\n",
      "Fitting estimator with 2018 features.\n",
      "Fitting estimator with 2008 features.\n",
      "Fitting estimator with 1998 features.\n",
      "Fitting estimator with 1988 features.\n",
      "Fitting estimator with 1978 features.\n",
      "Fitting estimator with 1968 features.\n",
      "Fitting estimator with 1958 features.\n",
      "Fitting estimator with 1948 features.\n",
      "Fitting estimator with 1938 features.\n",
      "Fitting estimator with 1928 features.\n",
      "Fitting estimator with 1918 features.\n",
      "Fitting estimator with 1908 features.\n",
      "Fitting estimator with 1898 features.\n",
      "Fitting estimator with 1888 features.\n",
      "Fitting estimator with 1878 features.\n",
      "Fitting estimator with 1868 features.\n",
      "Fitting estimator with 1858 features.\n",
      "Fitting estimator with 1848 features.\n",
      "Fitting estimator with 1838 features.\n",
      "Fitting estimator with 1828 features.\n",
      "Fitting estimator with 1818 features.\n",
      "Fitting estimator with 1808 features.\n",
      "Fitting estimator with 1798 features.\n",
      "Fitting estimator with 1788 features.\n",
      "2023-03-16 15:16:00,934 — INFO — Mols_shape: (500,)\n",
      "2023-03-16 15:16:00,935 — INFO — Features_shape: (500, 1778)\n",
      "2023-03-16 15:16:00,935 — INFO — Labels_shape: (500,)\n"
     ]
    },
    {
     "data": {
      "text/plain": "((500,), (500, 1778), (500,))"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from deepmol.feature_selection import RFECVFS\n",
    "\n",
    "d4 = deepcopy(csv_dataset)\n",
    "fs = RFECVFS(estimator=RandomForestClassifier(n_jobs=-1), # model to use\n",
    "             step=10, # number of features to remove at each step\n",
    "             min_features_to_select=1024, # minimum number of feature to keep (it can have more than that but never less)\n",
    "             cv=2, # number of folds in the cross validation\n",
    "             verbose=3) # verbosity level\n",
    "fs.select_features(d4)\n",
    "d4.get_shape() # 1778 features were kept"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Let's use the SelectFromModelFS feature selector\n",
    "\n",
    "SelectFromModel is a feature selection algorithm in machine learning that selects the most informative subset of features from a given dataset based on the importance scores provided by a base estimator.\n",
    "\n",
    "The algorithm works by training a machine learning model, such as a decision tree, random forest, or support vector machine, on the entire dataset and computing the importance score for each feature. The importance score reflects the contribution of each feature to the performance of the model.\n",
    "\n",
    "SelectFromModel then selects the top features based on a threshold value specified by the user. The threshold value can be an absolute value or a percentile of the importance scores. Features with importance scores higher than the threshold value are retained, while those with scores lower than the threshold value are discarded."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-03-16 15:22:08,826 — INFO — Mols_shape: (500,)\n",
      "2023-03-16 15:22:08,827 — INFO — Features_shape: (500, 273)\n",
      "2023-03-16 15:22:08,827 — INFO — Labels_shape: (500,)\n"
     ]
    },
    {
     "data": {
      "text/plain": "((500,), (500, 273), (500,))"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from deepmol.feature_selection import SelectFromModelFS\n",
    "\n",
    "d5 = deepcopy(csv_dataset)\n",
    "fs = SelectFromModelFS(estimator=RandomForestClassifier(n_jobs=-1), # model to use\n",
    "                       threshold=\"mean\") # Features whose importance is greater or equal are kept while the others are discarded. A percentil can also be used\n",
    "                                         # In this case (\"mean\") will keep the features with importance higher than the mean and remove the others\n",
    "fs.select_features(d5)\n",
    "d5.get_shape() # 273 features were kept"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Let's use the BorutaAlgorithm feature selector\n",
    "\n",
    "The boruta algorithm works by comparing the importance of each feature in the original dataset with the importance of the same feature in a shuffled version of the dataset. If the importance of the feature in the original dataset is significantly higher than its importance in the shuffled dataset, the feature is deemed \"confirmed\" and is selected for the final feature subset.\n",
    "\n",
    "It iteratively adds and removes features from the confirmed set until all features have been evaluated. The final set of confirmed features is the one that has a statistically significant higher importance in the original dataset compared to the shuffled dataset.\n",
    "\n",
    "The advantage of Boruta is that it can capture complex relationships between features and identify interactions that may not be apparent in simpler feature selection methods. It can also handle missing values and noisy data, which can be challenging for other feature selection techniques."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-03-16 15:32:33,628 — INFO — Mols_shape: (500,)\n",
      "2023-03-16 15:32:33,629 — INFO — Features_shape: (500, 72)\n",
      "2023-03-16 15:32:33,629 — INFO — Labels_shape: (500,)\n"
     ]
    },
    {
     "data": {
      "text/plain": "((500,), (500, 72), (500,))"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from deepmol.feature_selection import BorutaAlgorithm\n",
    "\n",
    "d6 = deepcopy(csv_dataset)\n",
    "fs = BorutaAlgorithm(estimator=RandomForestClassifier(n_jobs=-1), # model to use\n",
    "                     task='classification') # classification or regression\n",
    "fs.select_features(d6)\n",
    "d6.get_shape() # 72 features were kept"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
