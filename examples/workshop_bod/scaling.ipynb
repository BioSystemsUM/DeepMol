{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Data Scaling with DeepMol\n",
    "\n",
    "In machine learning and deep learning, scaling is important because it can significantly affect the performance of the algorithms. This is also true in the field of chemoinformatics, which involves the use of machine learning and other computational methods to analyze chemical data.\n",
    "\n",
    "One reason why scaling is important is that many machine learning algorithms use distance-based measures to calculate similarities between data points. If the features of the data are not scaled, the algorithm may give more weight to features with larger values, even if they are not more important for the analysis. This can lead to biased results and suboptimal model performance.\n",
    "\n",
    "Another reason why scaling is important is that it can help to speed up the training process. When the features of the data are not scaled, the optimization algorithm used in training may take longer to converge or may even fail to converge at all. This can be especially problematic in deep learning, where large amounts of data and complex models are often used.\n",
    "\n",
    "As we will see below, DeepMol offers a wide variety of scalers."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Let's start by loading some data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-03-16 16:09:02,739 — INFO — Assuming classification since there are less than 10 unique y values. If otherwise, explicitly set the mode to 'regression'!\n"
     ]
    },
    {
     "data": {
      "text/plain": "<deepmol.datasets.datasets.SmilesDataset at 0x7fb6e5d9e880>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from deepmol.compound_featurization import TwoDimensionDescriptors\n",
    "from deepmol.loaders import CSVLoader\n",
    "\n",
    "# Load data from CSV file\n",
    "loader = CSVLoader(dataset_path='../data/CHEMBL217_reduced.csv',\n",
    "                   smiles_field='SMILES',\n",
    "                   id_field='Original_Entry_ID',\n",
    "                   labels_fields=['Activity_Flag'],\n",
    "                   mode='auto',\n",
    "                   shard_size=2500)\n",
    "# create the dataset\n",
    "data = loader.create_dataset(sep=',', header=0)\n",
    "# create the features\n",
    "TwoDimensionDescriptors().featurize(data)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[11.368911743164062, 0.1637962907552719, 11.368911743164062, ...,\n        0.0, 0.0, 0.0],\n       [5.636220455169678, 1.0369679927825928, 5.636220455169678, ...,\n        0.0, 0.0, 0.0],\n       [12.955147743225098, -4.388545513153076, 12.955147743225098, ...,\n        0.0, 0.0, 0.0],\n       ...,\n       [11.333108901977539, -3.2535386085510254, 11.333108901977539, ...,\n        0.0, 2.0, 0.0],\n       [11.60878849029541, -0.683738112449646, 11.60878849029541, ...,\n        0.0, 1.0, 1.0],\n       [13.40140151977539, -4.504744052886963, 13.40140151977539, ...,\n        0.0, 1.0, 0.0]], dtype=object)"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view the features\n",
    "data.X # data is very heterogeneous"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### StandardScaler\n",
    "\n",
    "Standardize features by removing the mean and scaling to unit variance."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[-1.8814052765435616, 0.8124672709922676, -1.8814052765435616,\n        ..., -0.2641249606665856, -0.3210389365539343,\n        -0.22847842596169518],\n       [0.7200307678683329, 0.1934056547938061, 0.7200307678683329, ...,\n        3.4876500488019597, -0.3210389365539343, -0.22847842596169518],\n       [-0.7480856911706688, 0.8631871450036691, -0.7480856911706688,\n        ..., -0.2641249606665856, -0.3210389365539343,\n        -0.22847842596169518],\n       ...,\n       [0.8868328020377474, -0.043429240843857846, 0.8868328020377474,\n        ..., -0.2641249606665856, -0.3210389365539343,\n        -0.22847842596169518],\n       [0.598820137203169, -1.9514146621882777, 0.598820137203169, ...,\n        -0.2641249606665856, -0.3210389365539343, 4.304823676452892],\n       [-0.2938825728707849, 0.5765268719999578, -0.2938825728707849,\n        ..., -0.2641249606665856, -0.3210389365539343,\n        -0.22847842596169518]], dtype=object)"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "from deepmol.scalers import StandardScaler\n",
    "\n",
    "d1 = deepcopy(data)\n",
    "scaler = StandardScaler() # Standardize features by removing the mean and scaling to unit variance.\n",
    "scaler.fit_transform(d1) # you can scale only a portion of the data by passing a columns argument with the indexes of the columns to scale\n",
    "d1.X # the data is much more homogeneous"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### MinMaxScaler\n",
    "\n",
    "Transform features by scaling each feature to a given range."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[-0.8245046790614641, 1.5763108457360882, -0.8245046790614641,\n        ..., -2.0, -2.0, -2.0],\n       [1.40658061281867, 1.0304801559790957, 1.40658061281867, ..., 0.0,\n        -2.0, -2.0],\n       [0.1474710354123383, 1.6210308915144647, 0.1474710354123383, ...,\n        -2.0, -2.0, -2.0],\n       ...,\n       [1.549636047869813, 0.8216612742040238, 1.549636047869813, ...,\n        -2.0, -2.0, -2.0],\n       [1.3026260082542134, -0.8606219934519088, 1.3026260082542134, ...,\n        -2.0, -2.0, 0.0],\n       [0.537011996429527, 1.3682806475235947, 0.537011996429527, ...,\n        -2.0, -2.0, -2.0]], dtype=object)"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from deepmol.scalers import MinMaxScaler\n",
    "\n",
    "d2 = deepcopy(data)\n",
    "scaler = MinMaxScaler(feature_range=(-2, 2))\n",
    "scaler.fit_transform(d2)\n",
    "d2.X # data is scaled between -2 and 2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### MaxAbsScaler\n",
    "\n",
    "Scale each feature by its maximum absolute value."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0.39679988485644213, 0.08044543051902527, 0.39679988485644213,\n        ..., 0.0, 0.0, 0.0],\n       [0.8732695876449605, -0.0844564194664471, 0.8732695876449605, ...,\n        0.5, 0.0, 0.0],\n       [0.6043746384879907, 0.09395588058839104, 0.6043746384879907, ...,\n        0.0, 0.0, 0.0],\n       ...,\n       [0.903820450433205, -0.14754305157197695, 0.903820450433205, ...,\n        0.0, 0.0, 0.0],\n       [0.8510690829306901, -0.6557805476343079, 0.8510690829306901, ...,\n        0.0, 0.0, 0.5],\n       [0.6875648538487872, 0.01759706896325184, 0.6875648538487872, ...,\n        0.0, 0.0, 0.0]], dtype=object)"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from deepmol.scalers import MaxAbsScaler\n",
    "\n",
    "d3 = deepcopy(data)\n",
    "scaler = MaxAbsScaler()\n",
    "scaler.fit_transform(d3)\n",
    "d3.X"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### RobustScaler\n",
    "\n",
    "Scale features using statistics that are robust to outliers."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[-2.6134645990788266, 0.8394302974050732, -2.6134645990788266,\n        ..., 0.0, 0.0, 0.0],\n       [0.36193190071270753, -0.2936846980711512, 0.36193190071270753,\n        ..., 1.0, 0.0, 0.0],\n       [-1.3172285702488262, 0.9322666941500451, -1.3172285702488262,\n        ..., 0.0, 0.0, 0.0],\n       ...,\n       [0.5527119853093991, -0.7271814045008571, 0.5527119853093991, ...,\n        0.0, 0.0, 0.0],\n       [0.22329705439643574, -4.2195104966380965, 0.22329705439643574,\n        ..., 0.0, 0.0, 1.0],\n       [-0.7977330511155114, 0.4075708554022563, -0.7977330511155114,\n        ..., 0.0, 0.0, 0.0]], dtype=object)"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from deepmol.scalers import RobustScaler\n",
    "\n",
    "d4 = deepcopy(data)\n",
    "scaler = RobustScaler()\n",
    "scaler.fit_transform(d4)\n",
    "d4.X # scaled data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Normalizer scaler\n",
    "\n",
    "Normalize samples individually to unit norm."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[6.44254027357967e-05, 4.964840021505967e-06,\n        6.44254027357967e-05, ..., 0.0, 0.0, 0.0],\n       [0.0017665474285718922, -6.494233594956049e-05,\n        0.0017665474285718922, ..., 0.0001332605029279036, 0.0, 0.0],\n       [0.0007717010358063958, 4.5602126294505956e-05,\n        0.0007717010358063958, ..., 0.0, 0.0, 0.0],\n       ...,\n       [5.117548901847158e-07, -3.175534777686797e-08,\n        5.117548901847158e-07, ..., 0.0, 0.0, 0.0],\n       [9.812862600438637e-06, -2.874138605440572e-06,\n        9.812862600438637e-06, ..., 0.0, 0.0, 7.595482817939696e-07],\n       [6.767479454945923e-06, 6.58372811082483e-08,\n        6.767479454945923e-06, ..., 0.0, 0.0, 0.0]], dtype=object)"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from deepmol.scalers import Normalizer\n",
    "\n",
    "d5 = deepcopy(data)\n",
    "scaler = Normalizer(norm='l2') # One of 'l1', 'l2' or 'max'. The norm to use to normalize each non-zero sample.\n",
    "scaler.fit_transform(d5)\n",
    "d5.X # scaled data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Binarizer scaler\n",
    "\n",
    "Binarize data (set feature values to 0 or 1) according to a threshold."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[1.0, 0.0, 1.0, ..., 0.0, 0.0, 0.0],\n       [1.0, 0.0, 1.0, ..., 0.0, 0.0, 0.0],\n       [1.0, 0.0, 1.0, ..., 0.0, 0.0, 0.0],\n       ...,\n       [1.0, 0.0, 1.0, ..., 0.0, 0.0, 0.0],\n       [1.0, 0.0, 1.0, ..., 0.0, 0.0, 0.0],\n       [1.0, 0.0, 1.0, ..., 0.0, 0.0, 0.0]], dtype=object)"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from deepmol.scalers import Binarizer\n",
    "\n",
    "d6 = deepcopy(data)\n",
    "scaler = Binarizer(threshold=1) # features higher than 10 are set to 1, features lower than 10 are set to 0\n",
    "scaler.fit_transform(d6)\n",
    "d6.X"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### QuantileTransformer\n",
    "\n",
    "The QuantileTransformer is a preprocessing method that transforms input data to have a specified probability distribution. This function maps the data to a uniform or normal distribution using the quantiles of the input data.\n",
    "\n",
    "This transformer is often useful when working with machine learning algorithms that are sensitive to the scale and distribution of the input data, such as neural networks. The QuantileTransformer is particularly useful when the input data has a highly skewed distribution, as it can transform the data to a more Gaussian distribution."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0.11917433442104693, 0.8648555822493716, 0.11917433442104693,\n        ..., 0.0, 0.0, 0.0],\n       [0.8528472467303082, 0.3393335432981923, 0.8528472467303082, ...,\n        0.964964964964965, 0.0, 0.0],\n       [0.16362506894666942, 0.8910209009209445, 0.16362506894666942,\n        ..., 0.0, 0.0, 0.0],\n       ...,\n       [0.9454604506602478, 0.23024182238998928, 0.9454604506602478, ...,\n        0.0, 0.0, 0.0],\n       [0.7398938381601365, 0.08446417944924096, 0.7398938381601365, ...,\n        0.0, 0.0, 0.974974974974975],\n       [0.24398724771572622, 0.7594453819546745, 0.24398724771572622,\n        ..., 0.0, 0.0, 0.0]], dtype=object)"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from deepmol.scalers import QuantileTransformer\n",
    "\n",
    "d7 = deepcopy(data)\n",
    "scaler = QuantileTransformer()\n",
    "scaler.fit_transform(d7)\n",
    "d7.X # scale data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### PowerTransformer\n",
    "\n",
    "The PowerTransformer is a preprocessing function in scikit-learn that applies a power transformation to make the data more Gaussian-like. It can be used for data with skewed distributions, as well as data that has a linear relationship with the target variable.\n",
    "\n",
    "The PowerTransformer applies either a Box-Cox transformation or a Yeo-Johnson transformation to the input data. The Box-Cox transformation is only applicable to positive data, while the Yeo-Johnson transformation can be applied to both positive and negative data."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[-1.7812105979050612, 1.1312492101750076, -1.7812105979050612,\n        ..., -0.2692602319338518, -0.4474112915536186,\n        -0.2294157338705618],\n       [0.8972931787503582, -0.17610324753953793, 0.8972931787503582,\n        ..., 3.7138792056315566, -0.4474112915536186,\n        -0.2294157338705618],\n       [-1.1917163643153352, 1.294512078069877, -1.1917163643153352, ...,\n        -0.2692602319338518, -0.4474112915536186, -0.2294157338705618],\n       ...,\n       [1.2758737854551978, -0.4573048058954823, 1.2758737854551978, ...,\n        -0.2692602319338518, -0.4474112915536186, -0.2294157338705618],\n       [0.6435515114197725, -1.6152456182659845, 0.6435515114197725, ...,\n        -0.2692602319338518, -0.4474112915536186, 4.35889894345763],\n       [-0.747532790543019, 0.4919605157143571, -0.747532790543019, ...,\n        -0.2692602319338518, -0.4474112915536186, -0.2294157338705618]],\n      dtype=object)"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from deepmol.scalers import PowerTransformer\n",
    "\n",
    "d8 = deepcopy(data)\n",
    "scaler = PowerTransformer(method='yeo-johnson', # The power transform method. Available methods are: 'yeo-johnson', works with positive and negative values; box-cox', only works with strictly positive values\n",
    "                          standardize=True) # apply zero mean, unit variance normalization to the transformed output\n",
    "scaler.fit_transform(d8)\n",
    "d8.X # scaled data"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
