{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "invalid-point",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install tensorflow==2.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "arabic-grill",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.0\n"
     ]
    }
   ],
   "source": [
    "from loaders.Loaders import CSVLoader\n",
    "from compoundFeaturization.rdkitFingerprints import MorganFingerprint\n",
    "from splitters.splitters import SingletaskStratifiedSplitter\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from models.sklearnModels import SklearnModel\n",
    "from metrics.Metrics import Metric\n",
    "from metrics.metricsFunctions import r2_score, mean_absolute_error, mean_squared_error, median_absolute_error\n",
    "\n",
    "import tensorflow as tf\n",
    "print(tf.version.VERSION)\n",
    "\n",
    "from parameterOptimization.HyperparameterOpt import HyperparamOpt_Valid, HyperparamOpt_CV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "included-picnic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mols_shape:  4294\n",
      "Features_shape:  X not defined!\n",
      "Labels_shape:  (4294,)\n"
     ]
    }
   ],
   "source": [
    "#Load Dataset\n",
    "dataset = CSVLoader(dataset_path='data/PC-3.csv', \n",
    "                    mols_field='smiles', \n",
    "                    labels_fields='pIC50')\n",
    "dataset = dataset.create_dataset()\n",
    "dataset.get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "abroad-situation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Featurizing datapoint 0\n",
      "Featurizing datapoint 1000\n",
      "Featurizing datapoint 2000\n",
      "Featurizing datapoint 3000\n",
      "Featurizing datapoint 4000\n",
      "Mols_shape:  4294\n",
      "Features_shape:  (4294, 1024)\n",
      "Labels_shape:  (4294,)\n"
     ]
    }
   ],
   "source": [
    "#Featurization\n",
    "dataset = MorganFingerprint().featurize(dataset)\n",
    "dataset.get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "australian-toner",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mols_shape:  2574\n",
      "Features_shape:  (2574, 1024)\n",
      "Labels_shape:  (2574,)\n",
      "Mols_shape:  858\n",
      "Features_shape:  (858, 1024)\n",
      "Labels_shape:  (858,)\n",
      "Mols_shape:  858\n",
      "Features_shape:  (858, 1024)\n",
      "Labels_shape:  (858,)\n"
     ]
    }
   ],
   "source": [
    "#Data Split\n",
    "splitter = SingletaskStratifiedSplitter()\n",
    "train_dataset, valid_dataset, test_dataset = splitter.train_valid_test_split(dataset=dataset, frac_train=0.6, \n",
    "                                                                             frac_valid=0.2, frac_test=0.2)\n",
    "\n",
    "train_dataset.get_shape()\n",
    "valid_dataset.get_shape()\n",
    "test_dataset.get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bacterial-story",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scikit-Learn Random Forest\n",
    "#rf = RandomForestRegressor()\n",
    "#model = SklearnModel(model=rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "improved-roulette",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1024\n"
     ]
    }
   ],
   "source": [
    "from models.kerasModels import KerasModel\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "\n",
    "input_dim = train_dataset.X.shape[1]\n",
    "print(input_dim)\n",
    "\n",
    "def build_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, activation='relu', input_shape=[1024]))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(1))\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.RMSprop(0.001)\n",
    "\n",
    "    model.compile(loss='mse', optimizer=optimizer, metrics=['mae', 'mse'])\n",
    "    return model\n",
    "\n",
    "model = KerasModel(build_model, mode='regression', epochs = 5, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "respected-preparation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing K-fold split\n",
      "Epoch 1/5\n",
      "287/287 [==============================] - 0s 2ms/step - loss: 1.4657 - mae: 0.8271 - mse: 1.4657\n",
      "Epoch 2/5\n",
      "287/287 [==============================] - 0s 2ms/step - loss: 0.4967 - mae: 0.5247 - mse: 0.4967\n",
      "Epoch 3/5\n",
      "287/287 [==============================] - 0s 1ms/step - loss: 0.3694 - mae: 0.4568 - mse: 0.3694\n",
      "Epoch 4/5\n",
      "287/287 [==============================] - 0s 1ms/step - loss: 0.3006 - mae: 0.4088 - mse: 0.3006\n",
      "Epoch 5/5\n",
      "287/287 [==============================] - 0s 2ms/step - loss: 0.2542 - mae: 0.3749 - mse: 0.2542\n",
      "Train Score: \n",
      "287/287 [==============================] - 0s 686us/step\n",
      "6.55 6.6052065\n",
      "<class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
      "ERROR:  invalid index to scalar variable.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'numpy.float32' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/data3/metrics/Metrics.py\u001b[0m in \u001b[0;36mcompute_singletask_metric\u001b[0;34m(self, y_true, y_pred, **kwargs)\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m             \u001b[0mmetric_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: invalid index to scalar variable.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-dfd86dc62554>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#cross validation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_validate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMetric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr2_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfolds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/data3/models/kerasModels.py\u001b[0m in \u001b[0;36mcross_validate\u001b[0;34m(self, dataset, metric, folds)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Train Score: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m             \u001b[0mtrain_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdummy_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m             \u001b[0mtrain_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_score\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0mavg_train_score\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtrain_score\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data3/models/Models.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, dataset, metrics, per_task_metrics, n_classes)\u001b[0m\n\u001b[1;32m    161\u001b[0m         return evaluator.compute_model_performance(metrics,\n\u001b[1;32m    162\u001b[0m                                                    \u001b[0mper_task_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mper_task_metrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m                                                    n_classes=n_classes)\n\u001b[0m\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_task_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data3/evaluator/Evaluator.py\u001b[0m in \u001b[0;36mcompute_model_performance\u001b[0;34m(self, metrics, per_task_metrics, n_classes)\u001b[0m\n\u001b[1;32m    149\u001b[0m                                             \u001b[0mper_task_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mper_task_metrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m                                             \u001b[0mn_tasks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_tasks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m                                             n_classes=n_classes)\n\u001b[0m\u001b[1;32m    152\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mper_task_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m                 \u001b[0mmultitask_scores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcomputed_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data3/metrics/Metrics.py\u001b[0m in \u001b[0;36mcompute_metric\u001b[0;34m(self, y_true, y_pred, n_tasks, n_classes, per_task_metrics, **kwargs)\u001b[0m\n\u001b[1;32m    177\u001b[0m             metric_value = self.compute_singletask_metric(y_task,\n\u001b[1;32m    178\u001b[0m                                                           \u001b[0my_pred_task\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m                                                           **kwargs)\n\u001b[0m\u001b[1;32m    180\u001b[0m             \u001b[0mcomputed_metrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetric_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m': \\n'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcomputed_metrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data3/metrics/Metrics.py\u001b[0m in \u001b[0;36mcompute_singletask_metric\u001b[0;34m(self, y_true, y_pred, **kwargs)\u001b[0m\n\u001b[1;32m    230\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ERROR: '\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m             \u001b[0;31m#deal with different shapes of the otput of predict and predict_proba\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m             \u001b[0my_pred_mod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalize_labels_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m             \u001b[0mmetric_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred_mod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmetric_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data3/utils/utils.py\u001b[0m in \u001b[0;36mnormalize_labels_shape\u001b[0;34m(y_pred)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m                 \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'numpy.float32' has no len()"
     ]
    }
   ],
   "source": [
    "#cross validation\n",
    "model.cross_validate(dataset, Metric(r2_score), folds=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "controlled-nation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model training\n",
    "model.fit(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "consistent-morgan",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [Metric(mean_absolute_error), Metric(mean_squared_error), Metric(median_absolute_error), Metric(r2_score)]\n",
    "print(\"#############################\")\n",
    "# evaluate the model\n",
    "print('Training Dataset: ')\n",
    "train_score = model.evaluate(train_dataset, metrics)\n",
    "print(\"#############################\")\n",
    "print('Validation Dataset: ')\n",
    "valid_score = model.evaluate(valid_dataset, metrics)\n",
    "print(\"#############################\")\n",
    "print('Test Dataset: ')\n",
    "test_score = model.evaluate(test_dataset, metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acceptable-thumbnail",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build a model function for hyperparameter optimization\n",
    "def rf_model_builder(n_estimators=10, max_features='auto', criterion='mse'):\n",
    "    rf_model = RandomForestRegressor(n_estimators=n_estimators, max_features=max_features,\n",
    "                                     criterion=criterion)\n",
    "    return rf_model\n",
    "\n",
    "params_dict_rf = {\"n_estimators\": [10, 100],\n",
    "                  \"max_features\": [\"auto\", \"sqrt\", \"log2\", None],\n",
    "                  \"criterion\": [\"mse\", \"mae\"]\n",
    "                  }\n",
    "    \n",
    "model = SklearnModel(rf_model_builder, 'regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "persistent-little",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameter Optimization\n",
    "#optimizer = HyperparamOpt_Valid(rf_model_builder)\n",
    "\n",
    "#best_rf, best_hyperparams, all_results = optimizer.hyperparam_search(params_dict_rf, \n",
    "#                                                                     train_dataset, \n",
    "#                                                                     valid_dataset, \n",
    "#                                                                     Metric(r2_score),\n",
    "#                                                                     n_iter_search=15)\n",
    "\n",
    "#print('#################')\n",
    "#print(best_hyperparams)\n",
    "#print(best_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abandoned-poison",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate model\n",
    "#best_rf.evaluate(test_dataset, metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "understood-experience",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameter Optimization with CV\n",
    "optimizer = HyperparamOpt_CV(rf_model_builder)\n",
    "\n",
    "best_rf, best_hyperparams, all_results = optimizer.hyperparam_search('sklearn',\n",
    "                                                                     params_dict_rf, \n",
    "                                                                     train_dataset,  \n",
    "                                                                     'r2', \n",
    "                                                                     cv=3,\n",
    "                                                                     n_iter_search=10,\n",
    "                                                                    n_jobs=8)\n",
    "\n",
    "print('#################')\n",
    "print(best_hyperparams)\n",
    "print(best_rf)\n",
    "\n",
    "#Evaluate model\n",
    "best_rf.evaluate(test_dataset, metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
