{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "clean-collect",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "#sess = tf.compat.v1.Session(config=config)\n",
    "sess =tf.compat.v1.InteractiveSession(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "automated-oriental",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1\n"
     ]
    }
   ],
   "source": [
    "from loaders.Loaders import CSVLoader\n",
    "from compoundFeaturization.rdkitFingerprints import MorganFingerprint\n",
    "from featureSelection.baseFeatureSelector import LowVarianceFS\n",
    "from splitters.splitters import SingletaskStratifiedSplitter\n",
    "from models.kerasModels import KerasModel\n",
    "from metrics.Metrics import Metric\n",
    "from metrics.metricsFunctions import f1_score, roc_auc_score, precision_score, accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "print(tf.version.VERSION)\n",
    "from tensorflow.keras.losses import binary_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "pursuant-delhi",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mols_shape:  23290\n",
      "Features_shape:  X not defined!\n",
      "Labels_shape:  (23290,)\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#Load Dataset\n",
    "dataset = CSVLoader(dataset_path='preprocessed_dataset_wfoodb.csv', \n",
    "                    mols_field='Smiles', \n",
    "                    labels_fields='Class', \n",
    "                    id_field='ID')#, shard_size=4000)\n",
    "dataset = dataset.create_dataset()\n",
    "print(dataset.get_shape())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "certain-ultimate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Featurizing datapoint 0\n",
      "Featurizing datapoint 1000\n",
      "Featurizing datapoint 2000\n",
      "Featurizing datapoint 3000\n",
      "Featurizing datapoint 4000\n",
      "Featurizing datapoint 5000\n",
      "Featurizing datapoint 6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RDKit ERROR: [14:32:33] Explicit valence for atom # 1 Cl, 4, is greater than permitted\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error in smile: O=[Cl]=O\n",
      "Featurizing datapoint 7000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RDKit ERROR: [14:32:37] Explicit valence for atom # 3 B, 4, is greater than permitted\n",
      "RDKit ERROR: [14:32:37] Explicit valence for atom # 1 Cl, 9, is greater than permitted\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error in smile: OB1O[B]2(O)OB(O)O[B](O)(O1)O2\n",
      "error in smile: O=[Cl-](=O)(=O)=O\n",
      "Featurizing datapoint 8000\n",
      "Featurizing datapoint 9000\n",
      "Featurizing datapoint 10000\n",
      "Featurizing datapoint 11000\n",
      "Featurizing datapoint 12000\n",
      "Featurizing datapoint 13000\n",
      "Featurizing datapoint 14000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RDKit ERROR: [14:32:55] Explicit valence for atom # 0 P, 11, is greater than permitted\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error in smile: [P](OCC=C(C)C)(OCC=C(C)C)(=O)(OP(OCC=C(C)C)(OCC=C(C)C)=O)(CC=C(C)C)(CC=C(C)C)(CC=C(C)C)(CC=C(C)C)(CC=C(C)C)CC=C(C)C\n",
      "Featurizing datapoint 15000\n",
      "Featurizing datapoint 16000\n",
      "Featurizing datapoint 17000\n",
      "Featurizing datapoint 18000\n",
      "Featurizing datapoint 19000\n",
      "Featurizing datapoint 20000\n",
      "Featurizing datapoint 21000\n",
      "Featurizing datapoint 22000\n",
      "Featurizing datapoint 23000\n",
      "Elements with indexes:  [6257, 7708, 7709, 14244]  were removed due to the presence of NAs!\n",
      "The elements in question are:  ['O=[Cl]=O' 'OB1O[B]2(O)OB(O)O[B](O)(O1)O2' 'O=[Cl-](=O)(=O)=O'\n",
      " '[P](OCC=C(C)C)(OCC=C(C)C)(=O)(OP(OCC=C(C)C)(OCC=C(C)C)=O)(CC=C(C)C)(CC=C(C)C)(CC=C(C)C)(CC=C(C)C)(CC=C(C)C)CC=C(C)C']\n",
      "Mols_shape:  23286\n",
      "Features_shape:  (23286, 1024)\n",
      "Labels_shape:  (23286,)\n"
     ]
    }
   ],
   "source": [
    "#Featurization\n",
    "dataset = MorganFingerprint().featurize(dataset)\n",
    "dataset.get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fourth-intervention",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mols_shape:  23286\n",
      "Features_shape:  (23286, 49)\n",
      "Labels_shape:  (23286,)\n"
     ]
    }
   ],
   "source": [
    "#Feature Selection\n",
    "dataset = LowVarianceFS(0.15).featureSelection(dataset)\n",
    "dataset.get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "further-heritage",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Split\n",
    "splitter = SingletaskStratifiedSplitter()\n",
    "train_dataset, valid_dataset, test_dataset = splitter.train_valid_test_split(dataset=dataset, frac_train=0.6, \n",
    "                                                                             frac_valid=0.2, frac_test=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "instructional-contributor",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, GaussianNoise, Conv1D, Flatten, Reshape\n",
    "from tensorflow.keras.optimizers import Adadelta, Adam, RMSprop\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "#Hyperparameters for the network\n",
    "#DENSE = 128\n",
    "#DROPOUT = 0.5\n",
    "#C1_K  = 8 #Number of kernels/feature extractors for first layer\n",
    "#C1_S  = 32 #Width of the convolutional mini networks\n",
    "#C2_K  = 16\n",
    "#C2_S  = 32\n",
    "\n",
    "#activation='relu'\n",
    "\n",
    "input_dim = train_dataset.X.shape[1]\n",
    "\n",
    "def make_cnn_model(input_dim=input_dim,\n",
    "                   g_noise = 0.05, \n",
    "                   DENSE=128, \n",
    "                   DROPOUT=0.5, \n",
    "                   C1_K=8, \n",
    "                   C1_S=32, \n",
    "                   C2_K=16, \n",
    "                   C2_S=32,\n",
    "                   activation='relu',\n",
    "                   loss='binary_crossentropy',\n",
    "                   optimizer='adadelta', \n",
    "                   learning_rate=0.01, \n",
    "                   metrics='accuracy'):\n",
    "    model = Sequential()\n",
    "    #Adding a bit of GaussianNoise also works as regularization\n",
    "    model.add(GaussianNoise(g_noise, input_shape=(input_dim,)))\n",
    "    #First two is number of filter + kernel size\n",
    "    model.add(Reshape((input_dim, 1)))\n",
    "    model.add(Conv1D(C1_K, (C1_S), activation=activation, padding=\"same\"))\n",
    "    model.add(Conv1D(C2_K, (C2_S), padding=\"same\", activation=activation))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dropout(DROPOUT))\n",
    "    model.add(Dense(DENSE, activation=activation))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    if optimizer=='adadelta':\n",
    "        opt = Adadelta(lr=learning_rate)\n",
    "    elif optimizer=='adam':\n",
    "        opt = Adam(lr=learning_rate)\n",
    "    elif optimizer=='rsmprop':\n",
    "        opt = RMSprop(lr=learning_rate)\n",
    "    else : \n",
    "        opt = optimizer\n",
    "\n",
    "    model.compile(loss=loss, optimizer=opt, metrics=metrics)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "blocked-flashing",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from models.kerasModels import KerasModel\n",
    "\n",
    "#input_dim = train_dataset.X.shape[1]\n",
    "#print(input_dim)\n",
    "#model = KerasModel(make_cnn_model, epochs = 150, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aggregate-orlando",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#print(train_dataset.X.shape, train_dataset.y.shape)\n",
    "\n",
    "\n",
    "#model.fit(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "deadly-croatia",
   "metadata": {},
   "outputs": [],
   "source": [
    "#metrics = [Metric(roc_auc_score), \n",
    "#           Metric(precision_score), \n",
    "#           Metric(accuracy_score), \n",
    "#           Metric(confusion_matrix), \n",
    "#           Metric(classification_report)]\n",
    "\n",
    "#print('training set score:', model.evaluate(train_dataset, metrics))\n",
    "#print('test set score:', model.evaluate(test_dataset, metrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "after-reading",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODE:  classification\n",
      "Fitting 15 random models from a space of 243 possible models.\n",
      "Fitting model 1/15\n",
      "hyperparameters: {'optimizer': 'adam', 'DROPOUT': 0.2, 'learning_rate': 0.001, 'activation': 'elu', 'g_noise': 0.05}\n",
      "437/437 [==============================] - 6s 2ms/step - loss: 0.2095 - accuracy: 0.9381\n",
      "expected str, bytes or os.PathLike object, not NoneType\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'_PredictScorer' object has no attribute '__name__'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-71f5b2d29326>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m best_model, best_hyperparams, all_results = optimizer.hyperparam_search(params_dict, train_dataset,\n\u001b[1;32m     19\u001b[0m                                                                         \u001b[0mvalid_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m                                                                         n_jobs=1, verbose=3)\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m#print('#################')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data3/parameterOptimization/HyperparameterOpt.py\u001b[0m in \u001b[0;36mhyperparam_search\u001b[0;34m(self, params_dict, train_dataset, valid_dataset, metric, n_iter_search, n_jobs, verbose, use_max, logdir, **kwargs)\u001b[0m\n\u001b[1;32m    249\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m                 \u001b[0mmultitask_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mvalid_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmultitask_scores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mhp_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_convert_hyperparam_dict_to_filename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhyper_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data3/models/Models.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, dataset, metrics, per_task_metrics, n_classes)\u001b[0m\n\u001b[1;32m    161\u001b[0m         return evaluator.compute_model_performance(metrics,\n\u001b[1;32m    162\u001b[0m                                                    \u001b[0mper_task_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mper_task_metrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m                                                    n_classes=n_classes)\n\u001b[0m\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_task_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data3/evaluator/Evaluator.py\u001b[0m in \u001b[0;36mcompute_model_performance\u001b[0;34m(self, metrics, per_task_metrics, n_classes)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m          \u001b[0;31m# Process input metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m         \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_metric_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data3/evaluator/Evaluator.py\u001b[0m in \u001b[0;36m_process_metric_input\u001b[0;34m(metrics)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m             \u001b[0mwrap_metric\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMetric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"metric-%d\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m             \u001b[0mfinal_metrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrap_metric\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data3/metrics/Metrics.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, metric, task_averager, name, mode, n_tasks, classification_handling_mode, threshold_value)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0;31m# Some default metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             if self.metric.__name__ in [\"roc_auc_score\",\n\u001b[0m\u001b[1;32m     75\u001b[0m                                         \u001b[0;34m\"matthews_corrcoef\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m                                         \u001b[0;34m\"recall_score\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: '_PredictScorer' object has no attribute '__name__'"
     ]
    }
   ],
   "source": [
    "from parameterOptimization.HyperparameterOpt import HyperparamOpt_Valid\n",
    "from sklearn.metrics import f1_score, make_scorer\n",
    "\n",
    "\n",
    "#Hyperparameter Optimization\n",
    "optimizer = HyperparamOpt_Valid(make_cnn_model)\n",
    "\n",
    "params_dict = {'optimizer' : ['adam', 'rmsprop', 'adadelta'],\n",
    "              'DROPOUT' : [0.2, 0.4, 0.5],\n",
    "              'learning_rate' : [0.01, 0.001, 0.0001],\n",
    "              'activation' : ['relu', 'elu', 'selu'],\n",
    "              'g_noise' : [0.01, 0.05, 0.005]}\n",
    "\n",
    "#TODO: multiple scoring not working\n",
    "#scoring = {'f1': make_scorer(f1_score), 'Accuracy': 'accuracy'}\n",
    "scoring = make_scorer(f1_score)\n",
    "\n",
    "best_model, best_hyperparams, all_results = optimizer.hyperparam_search(params_dict, train_dataset,\n",
    "                                                                        valid_dataset, scoring,\n",
    "                                                                        n_jobs=1, verbose=3)\n",
    "\n",
    "#print('#################')\n",
    "#print(best_hyperparams)\n",
    "#print(best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "american-summit",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "sorted(sklearn.metrics.SCORERS.keys())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
